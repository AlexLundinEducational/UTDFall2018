Chapter 3 (page 77)
Solving by searching
non deterministic - program yeilds different results on different runs

3.1 Problem solving agents
	Goal - set of world states
	actions - cause transitions of world states
	Process
		1 goal formulation
			determines the stopping point for searching
		2 problem formulation
			determines what actions and states to condiser
		3 search
			looks for sequence of actions that results in 
			transition from intial state to goal
		4 execution
			follow sequence to goal
			
3.2 Formulating problems
	Knowledge amounts:
		Single state
			agents knows all effects of it's actions
			agent has all knowledge of world
				*simplest, search results in solution
		Multi state
			agents knows all effects of it's actions
			agent has some knowledge of world
				*plan ahead
		Contigency
			agents actions are non deterministic
			agent is missing some knowledge of world
				*scan during execution
				*agent can act before search returns solution
		Exploration
			agent is completely unware of results of its actions
			agent is missing all knowledge of world
3.3 Example problems

	problem - collection of information that the agent will
		use to decide what to do
		information:
			1 initial state - the starting space that the agent knows
				iteself to be in
				
			2 operator set - collection of actions, which transitions the agent to a state

			3 goal test - determines if solution is reached
			4 path cost - prioritizes solutions
			
	state space - all reachable states
	path - route from initial to solution
	
	Multistate probs:
		initial state SET
		operator set - action might reach a SET of states, rather than just one
		path - connects a SET of states
		solution - a path that leads to a SET of goal states
		
	Measuring
		1 solution or no?
		2 path cost
		3 search cost
		4 total cost
		
3.4 Searching for solutions 97

3.5 Search strategies

3.6 Avoiding repeated states

3.7 Constraint Satisfaction Search

3.8 Summary (107)
	Why is searching valueable?
		Because it gives a solution when the agent is not clear
		on which immediate action is best. The search returns
		a possible sequence of actions for the agent to 
		consider.
	
	Definitions:
		problem
			1 intial state 
			2 set of operators
			3 goal test function
			4 path cost function
			
		environment - represents the problem state space
		path - route through state space, from initial state to solution
		breadth first - expands shallowest node first
		uniform cost - expands least cost node first
		depth first - expands deepest node first
		depth limited - place limit on how deep a depth first search can go
		iterative deepening - depth-limited with increasing limt until goal is reached
		bidirectional - reduces time complexity, increases memory useage
		
	Evaluation:
		completeness
		optimality
		time complexity
		space complexity