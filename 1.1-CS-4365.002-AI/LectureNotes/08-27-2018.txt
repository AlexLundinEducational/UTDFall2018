Chapter 2
Omniscience, Learning, and Autonomy
	
	1.3 Nature of Environments
		Example in class of vaccum moving between 2 squares
		
		Task Environment represented by PEAS
			1 Performance
			2 Environment
			3 Actuators
			4 Sensors
		
		Properties
			1 Fully observable vs Partially observable
				Notes:
					Knowledge of the environment is not the same thing
					as having observations from sensors.
					Brute Force Learning a layout by moving repetativley is not observations
					from a sensor. 
					Example in class
					-(4 quadrant cicle with star and circle symbol)
					-(circles goal is to land in quadrant with star, atleast once)
					-(move into all 4 squares atleast once, then goal is achieved)
					-(no sensory actions needed at all)
				Definition:
					An agents sensors give it access to the complete state
					of the environment
				Reasons:
					Noisy, inaccurate sensors
					Non trivial, large environment
					*Environment is non observable AND agent has NO sensors
					
			2 Single Agent vs Multiagent
				Notes:
					We don't know exactly, how the other agent makes decisions
					We can assume the other agents goals, by evaluating performace measures
					
				Definition:
					Agent			- makes decisions
					Entity 			- obstacle in the environment 
									- does not make decisions
									- not always predictable (weather patterns)
					Zero sum game 	- only one winner
				Reasons:
				
			3 Deterministic vs Stochastic
				Notes:
				Definition:
					Deterministic	- next state is always predictable by
									- the current state AND the agents actions
					Stochastic		- stat is randomnly determined
						- Nondeterministic
							actions are characterized by possible outcomes.
							no probability attached.
						- Uncertain
				Reasons:
				
			4 Episodic vs Sequential
				Notes:
				Definition:
				Reasons:
							
			5 Static vs Dynamic
				Notes:
				Definition:
					Dynamic			- environment changes with time
					Static			- environment stays the same
					Semi dynamic	- environment does not change with time
									- environment changes with agents actions
				Reasons:
							
			6 Discrete vs Continuous
				Notes:
					applies to state of the environment
					considers how time is handled
					percepts and actions of the agent
				Example:
					Chess (not speed chess, moves are not timed)
						finite number of states
						discrete set of percepts and actions
					Taxi
						continuos state
						continuous time
				Definition:
				Reasons:
							
			7 Known vs Unknown
				Notes:
					This referes to the:
						designer's knowledge about the "laws of physics"
						about the environment
						
				Definition:
					Known
						Agent has the outcomes of the environment accounted for
							probabilities
					Unknown	
						Agent will have to learn
				Reasons:
				